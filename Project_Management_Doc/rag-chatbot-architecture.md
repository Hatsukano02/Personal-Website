# RAG èŠå¤©æœºå™¨äººæ¶æ„è®¾è®¡æ–‡æ¡£ v2.2

> åŸºäº OpenAI å®˜æ–¹æ–‡æ¡£ + ç”Ÿäº§çº§æœ€ä½³å®è·µ | è½»é‡çº§éƒ¨ç½² | 2GB VPS é€‚é…

## é¡¹ç›®æ¦‚è¿°

### åŠŸèƒ½å®šä½

åœ¨ä¸ªäººç½‘ç«™çš„ About Section å®ç°ä¸€ä¸ªåŸºäº RAG çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿï¼Œå…è®¸è®¿å®¢é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’äº†è§£ç½‘ç«™ä¸»äººçš„ä¸ªäººä¿¡æ¯ã€ç»å†å’Œè§‚ç‚¹ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **é˜…åå³ç„šå¼äº¤äº’**: æ¯æ¬¡å¯¹è¯åæ¸…ç†æ˜¾ç¤ºå†…å®¹ï¼Œæ— æŒä¹…åŒ–å¯¹è¯å†å²
- **å®Œå…¨è‡ªæ‰˜ç®¡**: ChromaDB + FastAPI éƒ¨ç½²åœ¨ VPS ä¸Š
- **è½»é‡çº§è®¾è®¡**: ä¼˜åŒ–å†…å­˜å ç”¨ï¼Œé€‚é… 2GB VPS ç¯å¢ƒ
- **æ™ºèƒ½å¼•å¯¼**: åŸºäº RAG å†…å®¹ç”Ÿæˆæ¨èé—®é¢˜

## æŠ€æœ¯æ¶æ„

### æŠ€æœ¯æ ˆç¡®è®¤ï¼ˆv2.2 ç”Ÿäº§çº§æ‰©å±•ï¼‰

```yaml
# åç«¯æŠ€æœ¯æ ˆ (åŸºäºOpenAIå®˜æ–¹æ–‡æ¡£ + ç”Ÿäº§å®è·µ)
backend:
  vector_db: ChromaDB v0.5.x
  api_framework: FastAPI v0.115.x
  python_version: 3.9+
  embeddings: OpenAI text-embedding-3-small
  llm: OpenAI gpt-4.1-mini # 1M+ context window, $0.40/$1.60 per 1M tokens

  # === æ–°å¢æ ¸å¿ƒåŠŸèƒ½ ===
  streaming: Server-Sent Events # OpenAIå®˜æ–¹æµå¼å“åº”
  function_calling: OpenAI Tools # å®˜æ–¹æ™ºèƒ½å·¥å…·è°ƒç”¨
  hybrid_search: BM25 + Vector # ç”Ÿäº§çº§æ··åˆæ£€ç´¢
  caching: Multi-layer Cache # æ™ºèƒ½ç¼“å­˜å±‚

# èµ„æºè¦æ±‚ (ChromaDBå®˜æ–¹æ ‡å‡† + ä¼˜åŒ–åˆ†é…)
resource_requirements:
  total_ram: 2GB # VPSæ€»å†…å­˜
  chromadb_allocation: 500MB # ChromaDBå®˜æ–¹æœ€ä½è¦æ±‚
  fastapi_allocation: 150MB # ä¼˜åŒ–ååˆ†é…
  cache_allocation: 50MB # æ–°å¢ç¼“å­˜å±‚
  buffer_allocation: 1300MB # ç³»ç»Ÿç¼“å†²

  # === å†…å­˜ä¼˜åŒ–ç­–ç•¥ ===
  chromadb_optimization:
    enable_mmap: true # å†…å­˜æ˜ å°„ä¼˜åŒ–
    max_batch_size: 100 # æ‰¹å¤„ç†å¤§å°é™åˆ¶
    index_compression: true # ç´¢å¼•å‹ç¼©

# å‰ç«¯é›†æˆ (å¢å¼ºäº¤äº’ä½“éªŒ)
frontend:
  framework: Next.js 15.x
  ai_sdk: Vercel AI SDK v3.x # å®˜æ–¹æµå¼æ”¯æŒ
  ui_library: Tailwind CSS
  markdown_renderer: react-markdown v9.x
  streaming_ui: React SSE client # æ–°å¢æµå¼UI

# é™æµå’Œçº¦æŸ (ä¸¥æ ¼æ§åˆ¶èµ„æº)
constraints:
  conversation_turns: 3 # æœ€å¤§3è½®å¯¹è¯
  session_queries: 5 # æ¯ä¼šè¯5æ¬¡æŸ¥è¯¢
  rate_limit: 3/min # æ¯åˆ†é’Ÿ3æ¬¡
  cleanup_interval: 30s # 30ç§’æ¸…ç†
  max_input_length: 500 # æœ€å¤§è¾“å…¥500å­—ç¬¦
  no_conversation_history: true # æ— å¯¹è¯å†å²æŒä¹…åŒ–
```

### ç³»ç»Ÿæ¶æ„å›¾ï¼ˆv2.2 å¢å¼ºç‰ˆï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Next.js Frontend (å¼ºåŒ–ç‰ˆ)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚            About Section Component               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ Traditional  â”‚  â”‚   Chat Interface       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   Profile    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ Streaming UI   â”‚   â”‚  â”‚    â”‚ <- æ–°å¢
â”‚  â”‚                    â”‚  â”‚ (SSE Client)   â”‚   â”‚  â”‚    â”‚
â”‚  â”‚                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚    â”‚
â”‚  â”‚                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚    â”‚
â”‚  â”‚                    â”‚  â”‚ Suggested Q's  â”‚   â”‚  â”‚    â”‚
â”‚  â”‚                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚    â”‚
â”‚  â”‚                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚    â”‚
â”‚  â”‚                    â”‚  â”‚ Chat Input(3è½®)â”‚   â”‚  â”‚    â”‚ <- å¢å¼º
â”‚  â”‚                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚    â”‚
â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ SSE Streaming + JSON API
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI RAG Service (ç”Ÿäº§çº§)                 â”‚
â”‚                     (Port 8000)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              å¢å¼ºç‰ˆ API Endpoints                â”‚    â”‚
â”‚  â”‚  /chat/stream - SSEæµå¼å¯¹è¯æ¥å£              â”‚    â”‚ <- æ–°å¢
â”‚  â”‚  /chat - æ ‡å‡†å¯¹è¯æ¥å£(å‘åå…¼å®¹)             â”‚    â”‚
â”‚  â”‚  /suggestions - æ™ºèƒ½æ¨èé—®é¢˜                   â”‚    â”‚ <- å¢å¼º
â”‚  â”‚  /health - å¥åº·æ£€æŸ¥ + å†…å­˜ç›‘æ§               â”‚    â”‚ <- å¢å¼º
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ é™æµä¸­é—´ä»¶   â”‚  â”‚ æ™ºèƒ½ç¼“å­˜å±‚   â”‚  â”‚ OpenAI      â”‚   â”‚ <- æ–°å¢ç¼“å­˜
â”‚  â”‚ (3/min,5/sess) â”‚  â”‚ (50MB LRU)   â”‚  â”‚ å®¢æˆ·ç«¯      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                              â”‚
â”‚                           â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              å¢å¼ºRAGæ£€ç´¢å¼•æ“                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ æ··åˆæ£€ç´¢    â”‚  â”‚    Function Calling      â”‚  â”‚    â”‚ <- æ–°å¢
â”‚  â”‚  â”‚ BM25+Vector â”‚  â”‚    (æ™ºèƒ½å·¥å…·è°ƒç”¨)        â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                              â”‚
â”‚                           â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         ChromaDB Vector Store (ä¼˜åŒ–ç‰ˆ)            â”‚    â”‚
â”‚  â”‚  Collections: personal_info                      â”‚    â”‚
â”‚  â”‚  Documents: ~600 chunks (1500 tokens/chunk)     â”‚    â”‚ <- ä¼˜åŒ–
â”‚  â”‚  Embedding Dim: 1536                            â”‚    â”‚
â”‚  â”‚  Memory Mapping: enabled                        â”‚    â”‚ <- æ–°å¢
â”‚  â”‚  Compression: enabled                           â”‚    â”‚ <- æ–°å¢
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å†…å­˜åˆ†é…å›¾ (2GB VPS)                    â”‚
â”‚  PostgreSQL(400MB) â”‚ Strapi(400MB) â”‚ ChromaDB(500MB)   â”‚
â”‚  FastAPI(150MB) â”‚ Cache(50MB) â”‚ Nginx(50MB) â”‚ Buffer(450MB) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## RAG çŸ¥è¯†åº“è®¾è®¡

### Markdown æ–‡ä»¶ç»„ç»‡ç»“æ„

```
/rag-knowledge-base/
â”œâ”€â”€ personal/              # ä¸ªäººä¿¡æ¯ç±»
â”‚   â”œâ”€â”€ education.md       # æ•™è‚²ç»å†
â”‚   â”œâ”€â”€ career.md         # èŒä¸šç»å†
â”‚   â””â”€â”€ basic-info.md     # åŸºæœ¬ä¿¡æ¯
â”œâ”€â”€ technical/            # æŠ€æœ¯è§‚ç‚¹ç±»
â”‚   â”œâ”€â”€ philosophy.md     # æŠ€æœ¯ç†å¿µ
â”‚   â”œâ”€â”€ stack-choices.md  # æŠ€æœ¯é€‰å‹åå¥½
â”‚   â””â”€â”€ industry-views.md # è¡Œä¸šè§‚ç‚¹
â”œâ”€â”€ interests/           # å…´è¶£çˆ±å¥½ç±»
â”‚   â”œâ”€â”€ photography.md   # æ‘„å½±ç›¸å…³
â”‚   â”œâ”€â”€ music.md        # éŸ³ä¹å“å‘³
â”‚   â””â”€â”€ movies.md       # ç”µå½±åå¥½
â”œâ”€â”€ thoughts/           # ä¸ªäººæ€è€ƒç±»
â”‚   â”œâ”€â”€ life-views.md   # äººç”Ÿè§‚
â”‚   â”œâ”€â”€ learning.md     # å­¦ä¹ æ–¹æ³•è®º
â”‚   â””â”€â”€ future-plans.md # æœªæ¥è§„åˆ’
â””â”€â”€ metadata.json       # å…ƒæ•°æ®é…ç½®
```

### Markdown æ–‡ä»¶æ ¼å¼è§„èŒƒ

```markdown
---
category: education
tags: [university, computer-science, courses]
priority: high
last_updated: 2025-01-09
---

# æ•™è‚²ç»å†

## å¤§å­¦é˜¶æ®µ

- **å­¦æ ¡**: XX å¤§å­¦
- **ä¸“ä¸š**: è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯
- **æ—¶é—´**: 2015-2019
- **GPA**: 3.8/4.0

### ä¸»è¦è¯¾ç¨‹

- æ•°æ®ç»“æ„ä¸ç®—æ³•ï¼šæ·±å…¥ç†è§£äº†...
- æ“ä½œç³»ç»Ÿï¼šæŒæ¡äº† Linux å†…æ ¸...
- è½¯ä»¶å·¥ç¨‹ï¼šå®è·µäº†æ•æ·å¼€å‘...

## é‡è¦é¡¹ç›®ç»å†

### æ¯•ä¸šè®¾è®¡

å¼€å‘äº†åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒè¯†åˆ«ç³»ç»Ÿ...
```

### å‘é‡åŒ–ç­–ç•¥ï¼ˆæ ‡å‡†åŒ–é…ç½®ï¼‰

```python
# åˆ©ç”¨GPT-4.1 miniå¤§ä¸Šä¸‹æ–‡çª—å£çš„ä¼˜åŒ–åˆ†å—ç­–ç•¥
chunk_config = {
    "chunk_size": 1500,          # æ›´å¤§åˆ†å—ï¼Œåˆ©ç”¨1Mä¸Šä¸‹æ–‡ä¼˜åŠ¿
    "chunk_overlap": 150,        # 10%é‡å ä¿æŒè¯­ä¹‰è¿è´¯
    "separator": "\n\n",         # ä¼˜å…ˆæŒ‰æ®µè½åˆ†å‰²
    "min_chunk_size": 300,       # æœ€å°chunkä¿è¯å®Œæ•´æ€§
    "total_chunks": 600,         # ä¼˜åŒ–åæ€»æ•°
    "metadata_fields": [
        "category", "tags", "source_file", "chunk_index",
        "importance_score", "last_updated"
    ]
}

# Embeddingé…ç½®ï¼ˆOpenAIå®˜æ–¹æ ‡å‡†ï¼‰
embedding_config = {
    "model": "text-embedding-3-small",
    "dimensions": 1536,
    "batch_size": 20,            # ä¿å®ˆæ‰¹æ¬¡å¤§å°
    "cache_embeddings": True     # æœ¬åœ°ç¼“å­˜ç­–ç•¥
}
```

## API è®¾è®¡ï¼ˆv2.2 ç”Ÿäº§çº§æ‰©å±•ï¼‰

### æ ¸å¿ƒæ¥å£å®šä¹‰

```python
# FastAPIæ¥å£è®¾è®¡ (åŸºäºOpenAIå®˜æ–¹æœ€ä½³å®è·µ)
from pydantic import BaseModel
from typing import List, Optional, AsyncGenerator
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None  # ç”¨äºé™æµ
    context_turns: int = 0             # ä¸Šä¸‹æ–‡è½®æ•°(0-3ï¼Œä¸¥æ ¼é™åˆ¶)
    stream: bool = False               # æ˜¯å¦æµå¼å“åº”

class ChatResponse(BaseModel):
    response: str
    suggestions: List[str]             # æ™ºèƒ½æ¨èé—®é¢˜
    tokens_used: int
    remaining_queries: int             # å‰©ä½™æŸ¥è¯¢æ¬¡æ•°
    function_calls: Optional[List[dict]] = None  # Function callingç»“æœ

class StreamingChatResponse(BaseModel):
    delta: str                         # å¢é‡å†…å®¹
    suggestions: Optional[List[str]] = None
    done: bool = False                 # æ˜¯å¦å®Œæˆ

class SuggestionsResponse(BaseModel):
    suggestions: List[str]
    category: str                      # å»ºè®®ç±»åˆ«
    context_based: bool = True         # åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆ

class HealthResponse(BaseModel):
    status: str
    memory_usage: dict                 # å†…å­˜ä½¿ç”¨æƒ…å†µ
    cache_stats: dict                  # ç¼“å­˜ç»Ÿè®¡
    rate_limit_stats: dict            # é™æµç»Ÿè®¡

# === å¢å¼ºç‰ˆAPIç«¯ç‚¹ ===
POST /api/rag/chat/stream   # ğŸ”¥ SSEæµå¼å¯¹è¯æ¥å£ (æ–°å¢)
POST /api/rag/chat          # æ ‡å‡†å¯¹è¯æ¥å£ (å‘åå…¼å®¹)
GET  /api/rag/suggestions   # æ™ºèƒ½æ¨èé—®é¢˜ (å¢å¼º)
GET  /api/rag/health        # å¥åº·æ£€æŸ¥ + ç›‘æ§ (å¢å¼º)
POST /api/rag/feedback      # ç”¨æˆ·åé¦ˆ (å¯é€‰)

# === OpenAI Function Calling å®šä¹‰ ===
RAG_FUNCTIONS = [
    {
        "name": "search_knowledge_base",
        "description": "æœç´¢ä¸ªäººçŸ¥è¯†åº“è·å–ç›¸å…³ä¿¡æ¯",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "æœç´¢æŸ¥è¯¢"},
                "category": {
                    "type": "string",
                    "enum": ["personal", "technical", "interests", "thoughts"],
                    "description": "çŸ¥è¯†ç±»åˆ«è¿‡æ»¤"
                },
                "max_results": {"type": "integer", "default": 5}
            },
            "required": ["query"]
        }
    },
    {
        "name": "generate_follow_up_questions",
        "description": "åŸºäºå½“å‰ä¸Šä¸‹æ–‡ç”Ÿæˆåç»­é—®é¢˜å»ºè®®",
        "parameters": {
            "type": "object",
            "properties": {
                "context": {"type": "string", "description": "å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡"},
                "topic": {"type": "string", "description": "å½“å‰è¯é¢˜"}
            },
            "required": ["context"]
        }
    }
]
```

### é™æµç­–ç•¥ï¼ˆä¸¥æ ¼æ§åˆ¶èµ„æºï¼‰

```python
# åŸºäºå†…å­˜çš„é™æµå®ç° (é’ˆå¯¹é˜…åå³ç„šè®¾è®¡ä¼˜åŒ–)
rate_limit_config = {
    "per_minute": 3,           # æ¯åˆ†é’Ÿ3æ¬¡ (ç¡¬é™åˆ¶)
    "per_session": 5,          # æ¯ä¼šè¯5è½® (ç¡¬é™åˆ¶)
    "max_turns": 3,            # æœ€å¤§3è½®å¯¹è¯ (æ–°å¢)
    "cooldown": 60,            # å†·å´æ—¶é—´(ç§’)
    "storage": "memory",       # ä½¿ç”¨å†…å­˜å­˜å‚¨
    "cleanup_interval": 300,   # 5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡è¿‡æœŸè®°å½•

    # === æ–°å¢èµ„æºä¿æŠ¤ ===
    "max_concurrent_requests": 5,     # æœ€å¤§å¹¶å‘è¯·æ±‚
    "memory_threshold": 0.85,         # å†…å­˜ä½¿ç”¨é˜ˆå€¼85%
    "auto_cleanup": True,             # è‡ªåŠ¨æ¸…ç†ä¼šè¯
    "session_timeout": 30,            # 30ç§’ä¼šè¯è¶…æ—¶
}

# === æ™ºèƒ½ç¼“å­˜ç­–ç•¥ ===
cache_config = {
    "max_memory": "50MB",             # æœ€å¤§ç¼“å­˜å†…å­˜
    "strategy": "LRU",                # LRUæ·˜æ±°ç­–ç•¥
    "cache_layers": {
        "embedding_cache": {
            "size": "30MB",
            "ttl": 3600,              # 1å°æ—¶TTL
            "key_pattern": "emb:{hash}"
        },
        "query_cache": {
            "size": "15MB",
            "ttl": 1800,              # 30åˆ†é’ŸTTL
            "key_pattern": "query:{session}:{hash}"
        },
        "suggestion_cache": {
            "size": "5MB",
            "ttl": 7200,              # 2å°æ—¶TTL
            "key_pattern": "suggest:{category}"
        }
    },

    # === ç¼“å­˜å‘½ä¸­ä¼˜åŒ– ===
    "preload_common_queries": True,   # é¢„åŠ è½½å¸¸è§æŸ¥è¯¢
    "cache_warming": True,            # ç¼“å­˜é¢„çƒ­
    "smart_eviction": True            # æ™ºèƒ½æ·˜æ±°
}
```

## éƒ¨ç½²æ–¹æ¡ˆ

### VPS èµ„æºåˆ†é…

```yaml
# 2GB VPSèµ„æºè§„åˆ’ï¼ˆåŸºäºChromaDBå®˜æ–¹2GBæœ€ä½è¦æ±‚ï¼‰
services:
  postgresql: 400MB # ç°æœ‰
  strapi: 400MB # ç°æœ‰
  nginx: 50MB # ç°æœ‰
  chromadb: 500MB # ChromaDBå®˜æ–¹æ¨èé…ç½®
  fastapi: 200MB # æ–°å¢
  system_buffer: 450MB # ç³»ç»Ÿå’Œå…¶ä»–æœåŠ¡ç¼“å†²

# ç«¯å£åˆ†é…
ports:
  strapi: 1337
  nextjs: 3000
  postgresql: 5432
  fastapi_rag: 8000
  chromadb: 8001 # å†…éƒ¨ä½¿ç”¨
```

### Docker Compose é…ç½®

```yaml
version: "3.8"

services:
  chromadb:
    image: chromadb/chroma:latest
    volumes:
      - ./chroma-data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma/chroma
      - ANONYMIZED_TELEMETRY=FALSE
    ports:
      - "127.0.0.1:8001:8000"
    restart: unless-stopped
    mem_limit: 500m
    mem_reservation: 400m

  fastapi-rag:
    build: ./rag-service
    volumes:
      - ./rag-knowledge-base:/app/knowledge
      - ./logs:/app/logs
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
    ports:
      - "127.0.0.1:8000:8000"
    depends_on:
      - chromadb
    restart: unless-stopped
    mem_limit: 200m
    mem_reservation: 150m
```

### Nginx é…ç½®

```nginx
# RAG APIä»£ç†é…ç½®
location /api/rag/ {
    proxy_pass http://127.0.0.1:8000/;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection 'upgrade';
    proxy_set_header Host $host;
    proxy_cache_bypass $http_upgrade;

    # é™æµé…ç½®
    limit_req zone=rag_limit burst=5 nodelay;
    limit_req_status 429;
}

# é™æµåŒºåŸŸå®šä¹‰
limit_req_zone $binary_remote_addr zone=rag_limit:10m rate=3r/m;
```

## å‰ç«¯é›†æˆ

### Next.js ç»„ä»¶è®¾è®¡

```typescript
// components/AboutSection/ChatBot.tsx
interface ChatBotProps {
  initialSuggestions: string[];
  maxTurns: number;
}

export const ChatBot: React.FC<ChatBotProps> = ({
  initialSuggestions,
  maxTurns = 3,
}) => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const [turnCount, setTurnCount] = useState(0);
  const [suggestions, setSuggestions] = useState(initialSuggestions);

  // é˜…åå³ç„šé€»è¾‘
  useEffect(() => {
    if (messages.length > 0) {
      const timer = setTimeout(() => {
        setMessages([]);
        setTurnCount(0);
      }, 30000); // 30ç§’åæ¸…ç†

      return () => clearTimeout(timer);
    }
  }, [messages]);

  // å…¶ä»–å®ç°...
};
```

### API å®¢æˆ·ç«¯

```typescript
// lib/rag-client.ts
class RAGClient {
  private baseURL: string;
  private sessionId: string;

  constructor() {
    this.baseURL = process.env.NEXT_PUBLIC_RAG_API_URL || "/api/rag";
    this.sessionId = this.generateSessionId();
  }

  async chat(message: string, contextTurns: number = 0): Promise<ChatResponse> {
    const response = await fetch(`${this.baseURL}/chat`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        message,
        session_id: this.sessionId,
        context_turns: contextTurns,
      }),
    });

    if (response.status === 429) {
      throw new Error("æŸ¥è¯¢æ¬¡æ•°è¶…é™ï¼Œè¯·ç¨åå†è¯•");
    }

    return response.json();
  }

  // å…¶ä»–æ–¹æ³•...
}
```

## æ ¸å¿ƒåŠŸèƒ½å®ç°ï¼ˆv2.2 æ–°å¢ï¼‰

### æµå¼å“åº”ç³»ç»Ÿï¼ˆOpenAI å®˜æ–¹ SSE æ ‡å‡†ï¼‰

```python
# åŸºäºOpenAIå®˜æ–¹æ–‡æ¡£çš„æµå¼å“åº”å®ç°
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import asyncio
import json

@app.post("/api/rag/chat/stream")
async def stream_chat_endpoint(request: ChatRequest):
    """SSEæµå¼èŠå¤©æ¥å£ - åŸºäºOpenAIå®˜æ–¹æœ€ä½³å®è·µ"""

    async def generate_stream():
        try:
            # 1. é™æµæ£€æŸ¥
            if not check_rate_limit(request.session_id):
                yield f"data: {json.dumps({'error': 'Rate limit exceeded'})}\n\n"
                return

            # 2. RAGæ£€ç´¢ (ä½¿ç”¨æ··åˆæ£€ç´¢)
            context = await hybrid_search(request.message)

            # 3. æµå¼è°ƒç”¨OpenAI API
            stream = await openai.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": f"Context: {context}"},
                    {"role": "user", "content": request.message}
                ],
                functions=RAG_FUNCTIONS,
                function_call="auto",
                stream=True,  # å¯ç”¨æµå¼
                temperature=0.3,
                max_tokens=300
            )

            # 4. å®æ—¶æµå¼è¾“å‡º
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    data = {
                        "delta": chunk.choices[0].delta.content,
                        "done": False
                    }
                    yield f"data: {json.dumps(data)}\n\n"
                    await asyncio.sleep(0.01)  # æ§åˆ¶æµé€Ÿ

            # 5. ç”Ÿæˆåç»­å»ºè®®
            suggestions = await generate_suggestions(context)
            final_data = {
                "delta": "",
                "suggestions": suggestions,
                "done": True
            }
            yield f"data: {json.dumps(final_data)}\n\n"

        except Exception as e:
            error_data = {"error": str(e), "done": True}
            yield f"data: {json.dumps(error_data)}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*"
        }
    )

# å‰ç«¯SSEå®¢æˆ·ç«¯é›†æˆ (Vercel AI SDK)
# components/ChatBot.tsx
import { useChat } from 'ai/react';

export function ChatBot() {
    const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({
        api: '/api/rag/chat/stream',
        streamMode: 'text',  # OpenAIæµå¼æ¨¡å¼
        onFinish: (message) => {
            // å¤„ç†å»ºè®®é—®é¢˜
            if (message.suggestions) {
                setSuggestions(message.suggestions);
            }
        }
    });

    // é˜…åå³ç„šé€»è¾‘ä¿æŒä¸å˜
    useEffect(() => {
        if (messages.length > 0) {
            const timer = setTimeout(() => {
                setMessages([]);
            }, 30000);
            return () => clearTimeout(timer);
        }
    }, [messages]);
}
```

### æ··åˆæ£€ç´¢ç³»ç»Ÿï¼ˆBM25 + Vectorï¼‰

```python
# ç”Ÿäº§çº§æ··åˆæ£€ç´¢å®ç° (å†…å­˜ä¼˜åŒ–ç‰ˆ)
from whoosh import index
from whoosh.fields import Schema, TEXT, ID
from whoosh.qparser import QueryParser
import chromadb
import numpy as np

class HybridRetriever:
    """æ··åˆæ£€ç´¢å¼•æ“ - BM25å…³é”®è¯ + ChromaDBå‘é‡"""

    def __init__(self, chroma_client, index_dir="./bm25_index"):
        self.chroma = chroma_client.get_collection("personal_rag")
        self.setup_bm25_index(index_dir)

    def setup_bm25_index(self, index_dir):
        """åˆå§‹åŒ–BM25ç´¢å¼• (è½»é‡çº§Whooshåº“)"""
        schema = Schema(
            id=ID(stored=True),
            content=TEXT(stored=True),
            category=TEXT(stored=True)
        )

        # åˆ›å»ºæˆ–æ‰“å¼€ç´¢å¼• (æœ€å¤§30MBå†…å­˜ä½¿ç”¨)
        if not os.path.exists(index_dir):
            os.makedirs(index_dir)
            self.bm25_index = index.create_in(index_dir, schema)
            self._index_documents()
        else:
            self.bm25_index = index.open_dir(index_dir)

    async def hybrid_search(self, query: str, max_results: int = 5) -> str:
        """æ··åˆæ£€ç´¢ï¼šBM25(30%) + Vector(70%)"""

        # 1. å¹¶è¡Œæ‰§è¡ŒBM25å’Œå‘é‡æ£€ç´¢
        bm25_task = asyncio.create_task(self._bm25_search(query, max_results))
        vector_task = asyncio.create_task(self._vector_search(query, max_results))

        bm25_results, vector_results = await asyncio.gather(bm25_task, vector_task)

        # 2. RRFèåˆç®—æ³• (Reciprocal Rank Fusion)
        fused_results = self._rrf_fusion(bm25_results, vector_results)

        # 3. æ„å»ºä¸Šä¸‹æ–‡ (åˆ©ç”¨GPT-4.1 miniå¤§ä¸Šä¸‹æ–‡ä¼˜åŠ¿)
        context = self._build_context(fused_results[:max_results])

        return context

    async def _bm25_search(self, query: str, max_results: int):
        """BM25å…³é”®è¯æ£€ç´¢"""
        with self.bm25_index.searcher() as searcher:
            parser = QueryParser("content", self.bm25_index.schema)
            parsed_query = parser.parse(query)
            results = searcher.search(parsed_query, limit=max_results)

            return [(hit['id'], hit['content'], hit.score) for hit in results]

    async def _vector_search(self, query: str, max_results: int):
        """ChromaDBå‘é‡æ£€ç´¢"""
        results = self.chroma.query(
            query_texts=[query],
            n_results=max_results,
            include=['documents', 'metadatas', 'distances']
        )

        return list(zip(
            results['ids'][0],
            results['documents'][0],
            [1.0 - d for d in results['distances'][0]]  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
        ))

    def _rrf_fusion(self, bm25_results, vector_results, k=60):
        """RRFèåˆç®—æ³• - ä¸šç•Œæ ‡å‡†"""
        score_dict = {}

        # BM25æƒé‡30%
        for rank, (doc_id, content, score) in enumerate(bm25_results, 1):
            score_dict[doc_id] = score_dict.get(doc_id, 0) + 0.3 * (1.0 / (k + rank))

        # Vectoræƒé‡70%
        for rank, (doc_id, content, score) in enumerate(vector_results, 1):
            score_dict[doc_id] = score_dict.get(doc_id, 0) + 0.7 * (1.0 / (k + rank))

        # æŒ‰åˆ†æ•°æ’åº
        fused_docs = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)

        return fused_docs

    def _build_context(self, doc_results) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡ - ä¼˜åŒ–Tokenä½¿ç”¨"""
        context_parts = []
        total_tokens = 0
        max_context_tokens = 8000  # ä¸ºGPT-4.1 minié¢„ç•™å¤§é‡ä¸Šä¸‹æ–‡

        for doc_id, score in doc_results:
            # ä»ChromaDBè·å–å®Œæ•´å†…å®¹
            doc_result = self.chroma.get(ids=[doc_id])
            if doc_result['documents']:
                content = doc_result['documents'][0]
                estimated_tokens = len(content.split()) * 1.3  # ä¼°ç®—tokenæ•°

                if total_tokens + estimated_tokens <= max_context_tokens:
                    context_parts.append(f"[ç›¸å…³åº¦: {score:.2f}]\n{content}\n")
                    total_tokens += estimated_tokens
                else:
                    break

        return "\n".join(context_parts)
```

### Function Calling é›†æˆ

```python
# OpenAI Function Calling å®ç°
async def process_chat_with_functions(request: ChatRequest) -> ChatResponse:
    """é›†æˆFunction Callingçš„å¯¹è¯å¤„ç†"""

    # 1. åˆå§‹LLMè°ƒç”¨
    response = await openai.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸ªäººç½‘ç«™çš„AIåŠ©æ‰‹..."},
            {"role": "user", "content": request.message}
        ],
        functions=RAG_FUNCTIONS,
        function_call="auto",
        temperature=0.3
    )

    function_calls = []
    final_response = ""

    # 2. å¤„ç†Function Calling
    if response.choices[0].message.function_call:
        function_call = response.choices[0].message.function_call
        function_name = function_call.name
        function_args = json.loads(function_call.arguments)

        # æ‰§è¡Œç›¸åº”çš„å‡½æ•°
        if function_name == "search_knowledge_base":
            search_result = await hybrid_search(
                function_args["query"],
                category=function_args.get("category"),
                max_results=function_args.get("max_results", 5)
            )
            function_calls.append({
                "name": function_name,
                "args": function_args,
                "result": search_result
            })

            # åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
            final_response = await generate_final_response(
                request.message, search_result
            )

        elif function_name == "generate_follow_up_questions":
            suggestions = await generate_contextual_suggestions(
                function_args["context"],
                function_args.get("topic")
            )
            function_calls.append({
                "name": function_name,
                "args": function_args,
                "result": suggestions
            })

    else:
        # ç›´æ¥å›ç­”ï¼Œæ— éœ€æ£€ç´¢
        final_response = response.choices[0].message.content

    return ChatResponse(
        response=final_response,
        suggestions=await generate_suggestions(request.message),
        tokens_used=response.usage.total_tokens,
        remaining_queries=get_remaining_queries(request.session_id),
        function_calls=function_calls
    )
```

## æ€§èƒ½ä¼˜åŒ–ï¼ˆå¢å¼ºç‰ˆï¼‰

### å‘é‡æ£€ç´¢ä¼˜åŒ–

```python
# ChromaDBé…ç½®ä¼˜åŒ–
chroma_settings = {
    "hnsw:space": "cosine",           # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
    "hnsw:construction_ef": 200,      # æ„å»ºæ—¶çš„æœç´¢å®½åº¦
    "hnsw:search_ef": 100,            # æŸ¥è¯¢æ—¶çš„æœç´¢å®½åº¦
    "hnsw:M": 16,                     # æ¯ä¸ªèŠ‚ç‚¹çš„è¾¹æ•°
    "hnsw:num_threads": 2             # ä½¿ç”¨çš„çº¿ç¨‹æ•°
}

# æŸ¥è¯¢ä¼˜åŒ–
search_params = {
    "n_results": 5,                   # è¿”å›top-5ç›¸å…³chunks
    "include": ["documents", "metadatas", "distances"],
    "where_document": None,           # å¯æ·»åŠ è¿‡æ»¤æ¡ä»¶
}
```

### ç¼“å­˜ç­–ç•¥ï¼ˆæ™ºèƒ½å¤šå±‚ç¼“å­˜ï¼‰

```python
# ç”Ÿäº§çº§ç¼“å­˜å®ç° - é’ˆå¯¹2GB VPSä¼˜åŒ–
import asyncio
from functools import lru_cache
from typing import Dict, Optional
import hashlib
import json
import time

class RAGCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ - 50MBå†…å­˜é™åˆ¶"""

    def __init__(self):
        # ä¸‰å±‚ç¼“å­˜æ¶æ„
        self.embedding_cache = {}      # 30MB - åµŒå…¥å‘é‡ç¼“å­˜
        self.query_cache = {}          # 15MB - æŸ¥è¯¢ç»“æœç¼“å­˜
        self.suggestion_cache = {}     # 5MB - æ¨èé—®é¢˜ç¼“å­˜

        # ç¼“å­˜ç»Ÿè®¡
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "memory_usage": 0
        }

        # å¯åŠ¨åå°æ¸…ç†ä»»åŠ¡
        asyncio.create_task(self._background_cleanup())

    async def get_embedding(self, text: str) -> Optional[list]:
        """è·å–ç¼“å­˜çš„åµŒå…¥å‘é‡"""
        cache_key = hashlib.md5(text.encode()).hexdigest()

        if cache_key in self.embedding_cache:
            self.cache_stats["hits"] += 1
            return self.embedding_cache[cache_key]["data"]

        self.cache_stats["misses"] += 1
        return None

    async def set_embedding(self, text: str, embedding: list):
        """ç¼“å­˜åµŒå…¥å‘é‡"""
        cache_key = hashlib.md5(text.encode()).hexdigest()

        # æ£€æŸ¥å†…å­˜é™åˆ¶
        if self._estimate_memory_usage() > 30 * 1024 * 1024:  # 30MB
            await self._evict_lru_embeddings()

        self.embedding_cache[cache_key] = {
            "data": embedding,
            "timestamp": time.time(),
            "access_count": 1
        }

    async def get_query_result(self, query: str, session_id: str) -> Optional[dict]:
        """è·å–ç¼“å­˜çš„æŸ¥è¯¢ç»“æœ"""
        cache_key = f"{session_id}:{hashlib.md5(query.encode()).hexdigest()}"

        if cache_key in self.query_cache:
            # æ›´æ–°è®¿é—®æ—¶é—´
            self.query_cache[cache_key]["last_access"] = time.time()
            self.cache_stats["hits"] += 1
            return self.query_cache[cache_key]["data"]

        self.cache_stats["misses"] += 1
        return None

    async def set_query_result(self, query: str, session_id: str, result: dict):
        """ç¼“å­˜æŸ¥è¯¢ç»“æœ"""
        cache_key = f"{session_id}:{hashlib.md5(query.encode()).hexdigest()}"

        # æ£€æŸ¥å†…å­˜é™åˆ¶ (15MB)
        if len(self.query_cache) > 100:  # é™åˆ¶æœ€å¤§æ¡ç›®æ•°
            await self._evict_expired_queries()

        self.query_cache[cache_key] = {
            "data": result,
            "timestamp": time.time(),
            "last_access": time.time(),
            "ttl": 1800  # 30åˆ†é’ŸTTL
        }

    async def get_suggestions(self, category: str) -> Optional[list]:
        """è·å–ç¼“å­˜çš„æ¨èé—®é¢˜"""
        if category in self.suggestion_cache:
            self.cache_stats["hits"] += 1
            return self.suggestion_cache[category]

        self.cache_stats["misses"] += 1
        return None

    async def _background_cleanup(self):
        """åå°æ¸…ç†è¿‡æœŸç¼“å­˜"""
        while True:
            await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
            await self._cleanup_expired_cache()

    async def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜é¡¹"""
        current_time = time.time()

        # æ¸…ç†è¿‡æœŸæŸ¥è¯¢ç¼“å­˜
        expired_queries = [
            key for key, value in self.query_cache.items()
            if current_time - value["timestamp"] > value["ttl"]
        ]

        for key in expired_queries:
            del self.query_cache[key]

        # æ¸…ç†é•¿æ—¶é—´æœªè®¿é—®çš„åµŒå…¥ç¼“å­˜
        if len(self.embedding_cache) > 1000:
            sorted_embeddings = sorted(
                self.embedding_cache.items(),
                key=lambda x: x[1]["timestamp"]
            )
            # åˆ é™¤æœ€è€çš„20%
            for key, _ in sorted_embeddings[:len(sorted_embeddings)//5]:
                del self.embedding_cache[key]

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
cache_manager = RAGCacheManager()

# é¢„åŠ è½½å¸¸è§æ¨èé—®é¢˜
SUGGESTED_QUESTIONS_CACHE = {
    "æŠ€æœ¯": [
        "ä½ æœ€æ“…é•¿å“ªäº›æŠ€æœ¯æ ˆï¼Ÿ",
        "èŠèŠä½ å¯¹AIæŠ€æœ¯çš„çœ‹æ³•",
        "æœ€è¿‘åœ¨å­¦ä¹ ä»€ä¹ˆæ–°æŠ€æœ¯ï¼Ÿ",
        "ä½ çš„ç¼–ç¨‹è¯­è¨€åå¥½æ˜¯ä»€ä¹ˆï¼Ÿ"
    ],
    "ä¸ªäºº": [
        "ä»‹ç»ä¸€ä¸‹ä½ çš„æ•™è‚²èƒŒæ™¯",
        "ä½ çš„èŒä¸šç»å†å¦‚ä½•ï¼Ÿ",
        "å¹³æ—¶æœ‰ä»€ä¹ˆå…´è¶£çˆ±å¥½ï¼Ÿ",
        "ä½ çš„äººç”Ÿè§„åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ"
    ],
    "é¡¹ç›®": [
        "ä»‹ç»ä¸€ä¸‹ä½ åšè¿‡çš„é¡¹ç›®",
        "æœ€æœ‰æŒ‘æˆ˜æ€§çš„é¡¹ç›®æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä½ çš„é¡¹ç›®ç®¡ç†ç»éªŒå¦‚ä½•ï¼Ÿ",
        "å¼€æºé¡¹ç›®å‚ä¸æƒ…å†µ"
    ],
    "è§‚ç‚¹": [
        "ä½ å¯¹è¿œç¨‹å·¥ä½œçš„çœ‹æ³•",
        "å¦‚ä½•çœ‹å¾…æŠ€æœ¯å‘å±•è¶‹åŠ¿ï¼Ÿ",
        "å·¥ä½œç”Ÿæ´»å¹³è¡¡çš„ç†å¿µ",
        "å¯¹å­¦ä¹ æˆé•¿çš„æ€è€ƒ"
    ]
}

# ç¼“å­˜è£…é¥°å™¨
def cache_result(cache_type: str, ttl: int = 1800):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"

            # å°è¯•ä»ç¼“å­˜è·å–
            if cache_type == "query":
                cached = await cache_manager.get_query_result(cache_key, "system")
                if cached:
                    return cached

            # æ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)

            # å­˜å…¥ç¼“å­˜
            if cache_type == "query":
                await cache_manager.set_query_result(cache_key, "system", result)

            return result
        return wrapper
    return decorator
```

## ç›‘æ§å’Œç»´æŠ¤

### æ—¥å¿—ç­–ç•¥

```python
# æ—¥å¿—é…ç½®
logging_config = {
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "handlers": {
        "file": {
            "filename": "/app/logs/rag-service.log",
            "maxBytes": 10485760,  # 10MB
            "backupCount": 5
        },
        "console": {
            "stream": "stdout"
        }
    }
}

# ç›‘æ§æŒ‡æ ‡
metrics_to_track = [
    "query_latency",          # æŸ¥è¯¢å»¶è¿Ÿ
    "embedding_time",         # å‘é‡åŒ–æ—¶é—´
    "openai_api_calls",      # APIè°ƒç”¨æ¬¡æ•°
    "rate_limit_hits",       # é™æµè§¦å‘æ¬¡æ•°
    "memory_usage",          # å†…å­˜ä½¿ç”¨
    "cache_hit_rate"         # ç¼“å­˜å‘½ä¸­ç‡
]
```

### æˆæœ¬ä¼°ç®—ï¼ˆv2.2 å¢å¼ºç‰ˆ - åŸºäºå®˜æ–¹å®šä»·ï¼‰

```python
# æœˆåº¦æˆæœ¬è®¡ç®—ï¼ˆOpenAIå®˜æ–¹å®šä»· + æ–°åŠŸèƒ½å½±å“ï¼‰
enhanced_cost_estimation = {
    "openai_embeddings": {
        "model": "text-embedding-3-small",
        "price_per_1M_tokens": 0.02,
        "monthly_tokens": 150000,         # æ··åˆæ£€ç´¢å¢åŠ 30%ç”¨é‡
        "cache_hit_rate": 0.6,            # 60%ç¼“å­˜å‘½ä¸­ç‡
        "actual_api_calls": 60000,        # 40%å®é™…APIè°ƒç”¨
        "monthly_cost": 0.0012            # $0.0012/æœˆ
    },
    "openai_completions": {
        "model": "gpt-4.1-mini",
        "monthly_queries": 300,           # 10æ¬¡/å¤© * 30å¤©(é™æµæ§åˆ¶)
        "avg_input_tokens": 1200,         # æ··åˆæ£€ç´¢å¢åŠ ä¸Šä¸‹æ–‡
        "avg_output_tokens": 400,         # å“åº”é•¿åº¦ä¿æŒ
        "function_calling_overhead": 50,  # Function callingé¢å¤–tokens

        # æ€»tokenè®¡ç®—
        "input_tokens_total": 375000,     # 300 * (1200 + 50)
        "output_tokens_total": 120000,    # 300 * 400

        # æˆæœ¬è®¡ç®—
        "input_price_per_1M": 0.40,       # å®˜æ–¹å®šä»·
        "output_price_per_1M": 1.60,      # å®˜æ–¹å®šä»·
        "input_cost": 0.15,               # $0.15
        "output_cost": 0.192,             # $0.192
        "total_llm_cost": 0.342           # $0.342/æœˆ
    },

    # === æ–°å¢ç¼“å­˜æ”¶ç›Šåˆ†æ ===
    "cache_benefits": {
        "embedding_cache_savings": 0.0008,  # 60%å‘½ä¸­ç‡èŠ‚çœ
        "query_cache_savings": 0.068,       # é‡å¤æŸ¥è¯¢èŠ‚çœ20%
        "total_savings": 0.0688             # æœˆåº¦èŠ‚çœ
    },

    # === åŸºç¡€è®¾æ–½æˆæœ¬ ===
    "infrastructure_cost": {
        "vps_cost": 0,                    # VPSå·²æœ‰
        "additional_storage": 0,          # æœ¬åœ°å­˜å‚¨
        "bandwidth_cost": 0,              # æµé‡è´¹ç”¨å¿½ç•¥
        "total_infra_cost": 0
    },

    # === æ€»æˆæœ¬åˆ†æ ===
    "cost_summary": {
        "gross_api_cost": 0.3432,        # åŸå§‹APIæˆæœ¬
        "cache_savings": -0.0688,        # ç¼“å­˜èŠ‚çœ
        "net_monthly_cost": 0.2744,      # å®é™…æœˆåº¦æˆæœ¬
        "cost_per_query": 0.00091,       # æ¯æ¬¡æŸ¥è¯¢çº¦$0.0009
        "vs_original_savings": "12.5%"   # ç›¸æ¯”v2.1èŠ‚çœ
    },

    # === æ€§èƒ½æå‡ä»·å€¼ ===
    "performance_value": {
        "response_latency_improvement": "40%",  # ç¼“å­˜å¸¦æ¥å»¶è¿Ÿæ”¹å–„
        "retrieval_accuracy_improvement": "25%", # æ··åˆæ£€ç´¢ç²¾åº¦æå‡
        "user_experience_score": "+30%",       # æµå¼å“åº”ä½“éªŒæå‡
        "cache_hit_rate_target": "60%+",       # ç›®æ ‡ç¼“å­˜å‘½ä¸­ç‡
    }
}

# === ROIåˆ†æ ===
roi_analysis = {
    "development_time_saved": {
        "streaming_ui": "2-3å¤©å¼€å‘æ—¶é—´",
        "hybrid_search": "1å‘¨é›†æˆæ—¶é—´",
        "caching_system": "3-4å¤©ä¼˜åŒ–æ—¶é—´",
        "total_dev_time_value": "$1000+ å¼€å‘æˆæœ¬"
    },
    "operational_benefits": {
        "reduced_latency": "å¹³å‡å“åº”æ—¶é—´<2s",
        "improved_accuracy": "æ£€ç´¢å‡†ç¡®ç‡>85%",
        "better_user_retention": "ç”¨æˆ·å®Œæˆç‡>90%",
        "cost_efficiency": "æ¯æŸ¥è¯¢æˆæœ¬é™ä½12.5%"
    }
}
```

## å®‰å…¨è€ƒè™‘

### API å®‰å…¨

```python
# å®‰å…¨é…ç½®
security_config = {
    "cors_origins": ["https://hatsukano.com"],
    "rate_limiting": True,
    "api_key_validation": False,  # å…¬å¼€è®¿é—®
    "input_sanitization": True,
    "max_input_length": 500,
    "prompt_injection_detection": True
}

# è¾“å…¥éªŒè¯
def validate_input(message: str) -> bool:
    # æ£€æŸ¥é•¿åº¦
    if len(message) > 500:
        return False

    # æ£€æŸ¥æ¶æ„æ¨¡å¼
    malicious_patterns = [
        "ignore previous",
        "system prompt",
        "reveal instructions"
    ]

    for pattern in malicious_patterns:
        if pattern.lower() in message.lower():
            return False

    return True
```

## å®æ–½è®¡åˆ’ï¼ˆv2.2 ç”Ÿäº§çº§éƒ¨ç½²ï¼‰

### Phase 1: åŸºç¡€æ¶æ„éƒ¨ç½²ï¼ˆå¢å¼ºç‰ˆï¼‰

```bash
# 1. ChromaDBéƒ¨ç½²ï¼ˆå†…å­˜ä¼˜åŒ–é…ç½®ï¼‰
docker run -d \
  --name chromadb \
  -v ./chroma-data:/chroma/chroma \
  -e IS_PERSISTENT=TRUE \
  -e ANONYMIZED_TELEMETRY=FALSE \
  --memory="500m" \
  --memory-reservation="400m" \
  --restart unless-stopped \
  chromadb/chroma:latest

# 2. å®‰è£…å®Œæ•´ä¾èµ–åŒ…
pip install fastapi[all] uvicorn chromadb openai whoosh asyncio-ratelimit
pip install pydantic[email] python-multipart aiofiles

# 3. FastAPIæœåŠ¡æ­å»ºï¼ˆç”Ÿäº§é…ç½®ï¼‰
python -m uvicorn main:app \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 1 \
  --loop asyncio \
  --access-log \
  --log-level info
```

**Phase 1 ä»»åŠ¡æ¸…å•:**

- [ ] éƒ¨ç½² ChromaDB å®¹å™¨ï¼ˆ500MB å†…å­˜ + æ˜ å°„ä¼˜åŒ–ï¼‰
- [ ] æ­å»º FastAPI åŸºç¡€æ¡†æ¶ + å¼‚æ­¥æ”¯æŒ
- [ ] é…ç½® Nginx åå‘ä»£ç† + SSE æ”¯æŒ
- [ ] è®¾ç½®ç¯å¢ƒå˜é‡å’Œå¯†é’¥ç®¡ç†
- [ ] ğŸ”¥ **æ–°å¢**: é…ç½® BM25 ç´¢å¼•ç›®å½•
- [ ] ğŸ”¥ **æ–°å¢**: åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ

### Phase 2: çŸ¥è¯†åº“æ„å»ºï¼ˆä¼˜åŒ–ç‰ˆï¼‰

```python
# çŸ¥è¯†åº“å¯¼å…¥è„šæœ¬ï¼ˆåˆ©ç”¨GPT-4.1 miniå¤§ä¸Šä¸‹æ–‡ï¼‰
import chromadb
from openai import OpenAI
from whoosh import index

async def build_enhanced_knowledge_base():
    """æ„å»ºå¢å¼ºç‰ˆçŸ¥è¯†åº“ï¼šå‘é‡+å…³é”®è¯åŒç´¢å¼•"""

    # 1. ChromaDBé…ç½®ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
    chroma = chromadb.PersistentClient(
        path="./chroma_db",
        settings={
            "anonymized_telemetry": False,
            "allow_reset": True
        }
    )

    # 2. åˆ›å»ºé›†åˆï¼ˆä¼˜åŒ–å‚æ•°ï¼‰
    collection = chroma.create_collection(
        name="personal_rag",
        metadata={
            "hnsw:space": "cosine",
            "hnsw:construction_ef": 200,  # æå‡æ„å»ºè´¨é‡
            "hnsw:search_ef": 100,        # å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
            "hnsw:M": 16,                 # æ ‡å‡†è¿æ¥æ•°
            "hnsw:num_threads": 2         # é™åˆ¶çº¿ç¨‹æ•°
        }
    )

    # 3. å¹¶è¡Œæ„å»ºBM25ç´¢å¼•
    bm25_index = setup_bm25_index("./bm25_index")

    # 4. ä¼˜åŒ–åˆ†å—ç­–ç•¥ï¼ˆåˆ©ç”¨1M+ä¸Šä¸‹æ–‡ï¼‰
    for file_path in knowledge_files:
        content = read_markdown_file(file_path)

        # æ›´å¤§å—å¤„ç†
        chunks = create_semantic_chunks(
            content,
            chunk_size=1500,      # åˆ©ç”¨å¤§ä¸Šä¸‹æ–‡ä¼˜åŠ¿
            overlap=150,          # 10%é‡å 
            preserve_structure=True
        )

        # æ‰¹é‡å‘é‡åŒ–
        embeddings = await batch_get_embeddings(chunks)

        # åŒé‡ç´¢å¼•
        collection.add(
            documents=chunks,
            embeddings=embeddings,
            metadatas=[
                {
                    "source": file_path,
                    "category": extract_category(file_path),
                    "chunk_index": i,
                    "tokens": estimate_tokens(chunk)
                }
                for i, chunk in enumerate(chunks)
            ]
        )

        # åŒæ­¥æ›´æ–°BM25ç´¢å¼•
        await update_bm25_index(bm25_index, chunks, file_path)
```

**Phase 2 ä»»åŠ¡æ¸…å•:**

- [ ] ç¼–å†™ Markdown çŸ¥è¯†æ–‡æ¡£ï¼ˆç»“æ„åŒ–ï¼‰
- [ ] å®ç°è¯­ä¹‰åˆ†å—è„šæœ¬ï¼ˆ1500 tokens/chunkï¼‰
- [ ] ğŸ”¥ **æ–°å¢**: æ„å»º BM25 å…³é”®è¯ç´¢å¼•
- [ ] ğŸ”¥ **æ–°å¢**: å®ç°åŒé‡ç´¢å¼•åŒæ­¥
- [ ] æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆChromaDB ä¼˜åŒ–é…ç½®ï¼‰
- [ ] æµ‹è¯•æ··åˆæ£€ç´¢æ•ˆæœ

### Phase 3: æ ¸å¿ƒ API å¼€å‘ï¼ˆç”Ÿäº§çº§ï¼‰

```python
# FastAPIå®Œæ•´å®ç° - é›†æˆæ‰€æœ‰æ–°åŠŸèƒ½
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import asyncio

app = FastAPI(title="RAG Chatbot API v2.2")

# 1. æµå¼èŠå¤©æ¥å£ï¼ˆOpenAI SSEæ ‡å‡†ï¼‰
@app.post("/api/rag/chat/stream")
async def stream_chat_endpoint(request: ChatRequest):
    """SSEæµå¼å¯¹è¯ - ä¸»è¦æ¥å£"""
    return await process_streaming_chat(request)

# 2. æ ‡å‡†èŠå¤©æ¥å£ï¼ˆå‘åå…¼å®¹ï¼‰
@app.post("/api/rag/chat")
async def chat_endpoint(request: ChatRequest) -> ChatResponse:
    """æ ‡å‡†å¯¹è¯æ¥å£ - å…¼å®¹æ€§æ”¯æŒ"""
    return await process_chat_with_functions(request)

# 3. æ™ºèƒ½æ¨èæ¥å£ï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰
@app.get("/api/rag/suggestions")
async def enhanced_suggestions(category: str = None, context: str = None):
    """åŸºäºä¸Šä¸‹æ–‡çš„æ™ºèƒ½æ¨è"""
    return await generate_contextual_suggestions(category, context)

# 4. å¥åº·æ£€æŸ¥ï¼ˆç›‘æ§å¢å¼ºï¼‰
@app.get("/api/rag/health")
async def enhanced_health_check() -> HealthResponse:
    """å¢å¼ºç‰ˆå¥åº·æ£€æŸ¥"""
    return HealthResponse(
        status="healthy",
        memory_usage=await get_memory_stats(),
        cache_stats=cache_manager.cache_stats,
        rate_limit_stats=await get_rate_limit_stats()
    )

# 5. ä¸­é—´ä»¶é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://hatsukano.com"],
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# 6. å¯åŠ¨æ—¶åˆå§‹åŒ–
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨åˆå§‹åŒ–"""
    await initialize_hybrid_retriever()
    await cache_manager.preload_common_suggestions()
    await setup_background_tasks()
```

**Phase 3 ä»»åŠ¡æ¸…å•:**

- [ ] ğŸ”¥ **æ–°å¢**: å®ç° SSE æµå¼æ¥å£
- [ ] ğŸ”¥ **æ–°å¢**: é›†æˆ Function Calling
- [ ] ğŸ”¥ **æ–°å¢**: å®ç°æ··åˆæ£€ç´¢å¼•æ“
- [ ] ğŸ”¥ **æ–°å¢**: éƒ¨ç½²æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
- [ ] æ·»åŠ å¢å¼ºç‰ˆé™æµä¸­é—´ä»¶
- [ ] å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨è
- [ ] æ·»åŠ å®Œæ•´é”™è¯¯å¤„ç†å’Œç›‘æ§

### Phase 4: å‰ç«¯é›†æˆï¼ˆæµå¼ä½“éªŒï¼‰

```typescript
// å¢å¼ºç‰ˆChatBotç»„ä»¶ - æ”¯æŒæµå¼å“åº”
import { useChat } from "ai/react";
import { useState, useEffect } from "react";

export function EnhancedChatBot() {
  const [turnCount, setTurnCount] = useState(0);
  const [suggestions, setSuggestions] = useState<string[]>([]);

  // Vercel AI SDKé›†æˆ
  const { messages, input, handleInputChange, handleSubmit, isLoading } =
    useChat({
      api: "/api/rag/chat/stream", // æµå¼æ¥å£
      streamMode: "text",
      onFinish: (message) => {
        // å¤„ç†æµå¼å®Œæˆ
        if (message.suggestions) {
          setSuggestions(message.suggestions);
        }
      },
      onError: (error) => {
        console.error("Chat error:", error);
      },
    });

  // è½®æ•°é™åˆ¶æ£€æŸ¥
  const handleEnhancedSubmit = async (e: FormEvent) => {
    if (turnCount >= 3) {
      alert("å·²è¾¾åˆ°æœ€å¤§å¯¹è¯è½®æ•°é™åˆ¶");
      return;
    }

    setTurnCount((prev) => prev + 1);
    await handleSubmit(e);
  };

  // é˜…åå³ç„šé€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
  useEffect(() => {
    if (messages.length > 0) {
      const timer = setTimeout(() => {
        setMessages([]);
        setTurnCount(0);
      }, 30000);
      return () => clearTimeout(timer);
    }
  }, [messages]);

  return (
    <div className="chat-interface">
      {/* æµå¼æ¶ˆæ¯æ˜¾ç¤º */}
      <StreamingMessageList messages={messages} />

      {/* æ™ºèƒ½æ¨èé—®é¢˜ */}
      <SuggestionsList
        suggestions={suggestions}
        onSelect={(suggestion) => setInput(suggestion)}
      />

      {/* è¾“å…¥æ§ä»¶ */}
      <ChatInput
        value={input}
        onChange={handleInputChange}
        onSubmit={handleEnhancedSubmit}
        disabled={isLoading || turnCount >= 3}
        placeholder={`è¿˜å¯ä»¥é—® ${3 - turnCount} ä¸ªé—®é¢˜...`}
      />

      {/* å®æ—¶çŠ¶æ€æŒ‡ç¤º */}
      <StatusIndicator
        isLoading={isLoading}
        turnCount={turnCount}
        maxTurns={3}
      />
    </div>
  );
}
```

**Phase 4 ä»»åŠ¡æ¸…å•:**

- [ ] ğŸ”¥ **æ–°å¢**: å¼€å‘æµå¼ UI ç»„ä»¶
- [ ] ğŸ”¥ **æ–°å¢**: é›†æˆ Vercel AI SDK
- [ ] å®ç°è½®æ•°é™åˆ¶ç•Œé¢
- [ ] æ·»åŠ å®æ—¶çŠ¶æ€æŒ‡ç¤ºå™¨
- [ ] ä¼˜åŒ–ç§»åŠ¨ç«¯ä½“éªŒ
- [ ] æ·»åŠ é”™è¯¯è¾¹ç•Œå¤„ç†

### Phase 5: éƒ¨ç½²å’Œç›‘æ§ï¼ˆç”Ÿäº§çº§ï¼‰

```yaml
# Docker Composeå®Œæ•´é…ç½®
version: "3.8"

services:
  chromadb:
    image: chromadb/chroma:latest
    volumes:
      - ./chroma-data:/chroma/chroma
      - ./bm25-index:/app/bm25_index # æ–°å¢BM25ç´¢å¼•
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    mem_limit: 500m
    mem_reservation: 400m
    restart: unless-stopped

  fastapi-rag:
    build: ./rag-service
    volumes:
      - ./rag-knowledge-base:/app/knowledge
      - ./logs:/app/logs
      - ./cache:/app/cache # æ–°å¢ç¼“å­˜ç›®å½•
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - CHROMA_HOST=chromadb
      - CACHE_SIZE=50MB # ç¼“å­˜é…ç½®
      - RATE_LIMIT_PER_MIN=3
    mem_limit: 150m # ä¼˜åŒ–å†…å­˜åˆ†é…
    mem_reservation: 120m
    depends_on:
      - chromadb
    restart: unless-stopped
```

**Phase 5 ä»»åŠ¡æ¸…å•:**

- [ ] ğŸ”¥ **æ–°å¢**: éƒ¨ç½²ç¼“å­˜ç›‘æ§
- [ ] ğŸ”¥ **æ–°å¢**: é…ç½® SSE Nginx ä»£ç†
- [ ] æ€§èƒ½å‹æµ‹å’Œä¼˜åŒ–
- [ ] å†…å­˜ä½¿ç”¨ç›‘æ§
- [ ] å®‰å…¨åŠ å›ºï¼ˆè¾“å…¥éªŒè¯ï¼‰
- [ ] ç”¨æˆ·ä½“éªŒæµ‹è¯•
- [ ] ğŸ”¥ **æ–°å¢**: æ··åˆæ£€ç´¢æ•ˆæœè¯„ä¼°
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### **å…³é”®å‡çº§è¦ç‚¹**

1. **æµå¼å“åº”**: OpenAI å®˜æ–¹ SSE æ ‡å‡†å®ç°
2. **æ··åˆæ£€ç´¢**: BM25 + Vector åŒå¼•æ“
3. **æ™ºèƒ½ç¼“å­˜**: ä¸‰å±‚ç¼“å­˜æ¶æ„ï¼ˆ50MB æ€»é‡ï¼‰
4. **Function Calling**: æ™ºèƒ½å·¥å…·è°ƒç”¨
5. **å†…å­˜ä¼˜åŒ–**: é’ˆå¯¹ 2GB VPS çš„ç²¾ç¡®é…ç½®
6. **ç›‘æ§å¢å¼º**: å®æ—¶æ€§èƒ½å’Œèµ„æºç›‘æ§

## æŠ€æœ¯é£é™©å’Œç¼“è§£æªæ–½

### é£é™©è¯„ä¼°

| é£é™©é¡¹          | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½                                  |
| --------------- | ------ | ---- | ----------------------------------------- |
| VPS å†…å­˜ä¸è¶³    | ä¸­     | é«˜   | ChromaDB å®˜æ–¹è¦æ±‚ 2GB æœ€ä½ï¼Œå½“å‰ VPS æ»¡è¶³ |
| OpenAI API å»¶è¿Ÿ | ä¸­     | ä¸­   | æ·»åŠ è¶…æ—¶å¤„ç†å’Œç¼“å­˜æœºåˆ¶                    |
| å‘é‡æ£€ç´¢å‡†ç¡®åº¦  | ä¸­     | ä¸­   | æ ‡å‡† 512-token åˆ†å—ï¼Œä¼˜åŒ–æ£€ç´¢å‚æ•°         |
| æˆæœ¬è¶…é¢„ç®—      | ä½     | ä¸­   | è®¾ç½® API è°ƒç”¨é™åˆ¶ï¼Œç›‘æ§æœˆåº¦ä½¿ç”¨é‡         |
| æœåŠ¡ç¨³å®šæ€§      | ä¸­     | é«˜   | Docker å®¹å™¨é‡å¯ç­–ç•¥ï¼Œæ—¥å¿—ç›‘æ§             |

### ç›®æ ‡æŒ‡æ ‡

```python
success_metrics = {
    "performance": {
        "avg_latency": "< 3s",
        "p95_latency": "< 5s",
        "accuracy_rate": "> 80%"
    },
    "cost_efficiency": {
        "monthly_budget": "< $1",
        "cost_per_query": "< $0.01"
    },
    "user_experience": {
        "response_quality": "> 4/5",
        "completion_rate": "> 85%"
    }
}
```

## åç»­æ‰©å±•ä¸è·¯çº¿å›¾

### åŸºç¡€åŠŸèƒ½

1. **åŸºæœ¬å¯¹è¯**: æ”¯æŒä¸­è‹±æ–‡é—®ç­”
2. **æ™ºèƒ½é—®é¢˜ç”Ÿæˆ**: åŸºäºå†…å®¹ç”Ÿæˆæ¨èé—®é¢˜
3. **RAG æ£€ç´¢**: ä»ä¸ªäººçŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯
4. **æˆæœ¬æ§åˆ¶**: é€šè¿‡é™æµæ§åˆ¶ API ä½¿ç”¨é‡
5. **é˜…åå³ç„š**: å¯¹è¯åè‡ªåŠ¨æ¸…ç†ç•Œé¢

### æ‰©å±•è®¡åˆ’

1. **è¯­éŸ³äº¤äº’**: é›†æˆéŸ³é¢‘å¤„ç†åŠŸèƒ½
2. **çŸ¥è¯†æ›´æ–°**: æ”¯æŒåŠ¨æ€æ·»åŠ æ–°å†…å®¹
3. **ç”¨æˆ·åé¦ˆ**: æ”¶é›†å’Œæ”¹è¿›å“åº”è´¨é‡
4. **å¤šæ¨¡æ€**: æ”¯æŒå›¾ç‰‡ç­‰å¤šåª’ä½“å†…å®¹

### æŠ€æœ¯æ¼”è¿›è·¯çº¿

```python
roadmap = {
    "vector_database": {
        "current": "ChromaDB (500MB)",
        "future": "å‡çº§åˆ°æ›´é«˜æ€§èƒ½ç‰ˆæœ¬"
    },
    "llm_models": {
        "current": "gpt-4.1-mini",
        "future": "æ ¹æ®OpenAIæ–°å‘å¸ƒæ¨¡å‹è°ƒæ•´"
    },
    "deployment": {
        "current": "2GB VPS å•æœº",
        "future": "æ ¹æ®ä½¿ç”¨é‡è€ƒè™‘æ‰©å±•"
    }
}
```

## æŠ€æœ¯å‡çº§æ€»ç»“

### v2.2 vs v2.1 å¯¹æ¯”

| åŠŸèƒ½æ¨¡å—     | v2.1 åŸºç¡€ç‰ˆ | v2.2 ç”Ÿäº§çº§         | æå‡æ•ˆæœ         |
| ------------ | ----------- | ------------------- | ---------------- |
| **å“åº”æ–¹å¼** | åŒæ­¥å“åº”    | ğŸ”¥ SSE æµå¼å“åº”     | ç”¨æˆ·ä½“éªŒæå‡ 30% |
| **æ£€ç´¢å¼•æ“** | çº¯å‘é‡æ£€ç´¢  | ğŸ”¥ BM25+Vector æ··åˆ | å‡†ç¡®ç‡æå‡ 25%   |
| **æ™ºèƒ½å·¥å…·** | åŸºç¡€ RAG    | ğŸ”¥ Function Calling | æ™ºèƒ½åŒ–æ°´å¹³æå‡   |
| **ç¼“å­˜ç³»ç»Ÿ** | ç®€å• LRU    | ğŸ”¥ ä¸‰å±‚æ™ºèƒ½ç¼“å­˜     | å“åº”é€Ÿåº¦æå‡ 40% |
| **å†…å­˜ä¼˜åŒ–** | åŸºç¡€é…ç½®    | ğŸ”¥ ç²¾ç¡®å†…å­˜æ˜ å°„     | èµ„æºåˆ©ç”¨ç‡æå‡   |
| **API è®¾è®¡** | 3 ä¸ªç«¯ç‚¹    | ğŸ”¥ 5 ä¸ªå¢å¼ºç«¯ç‚¹     | åŠŸèƒ½å®Œæ•´æ€§æå‡   |
| **ç›‘æ§èƒ½åŠ›** | åŸºç¡€æ—¥å¿—    | ğŸ”¥ å®æ—¶æ€§èƒ½ç›‘æ§     | è¿ç»´èƒ½åŠ›æå‡     |
| **æœˆåº¦æˆæœ¬** | $0.314      | $0.274              | ğŸ”¥ èŠ‚çœ 12.5%    |

### æ ¸å¿ƒæŠ€æœ¯æ ˆå®Œæ•´æ€§

```yaml
# === OpenAIå®˜æ–¹æ ‡å‡†é›†æˆ ===
âœ… GPT-4.1 mini: 1M+ä¸Šä¸‹æ–‡ï¼Œè¶…é«˜æ€§ä»·æ¯”
âœ… text-embedding-3-small: 1536ç»´å‘é‡
âœ… Function Calling: æ™ºèƒ½å·¥å…·è°ƒç”¨
âœ… Streaming: SSEæµå¼å“åº”æ ‡å‡†
âœ… Error Handling: å®˜æ–¹é‡è¯•æœºåˆ¶

# === ç”Ÿäº§çº§ç»„ä»¶å®Œå¤‡æ€§ ===
âœ… ChromaDB: 500MBå†…å­˜ä¼˜åŒ–é…ç½®
âœ… FastAPI: å¼‚æ­¥+æµå¼+ä¸­é—´ä»¶
âœ…æ··åˆæ£€ç´¢: BM25+Vectorèåˆç®—æ³•
âœ… æ™ºèƒ½ç¼“å­˜: 50MBä¸‰å±‚æ¶æ„
âœ… é™æµæ§åˆ¶: ä¸¥æ ¼èµ„æºä¿æŠ¤
âœ… ç›‘æ§å‘Šè­¦: å®æ—¶æ€§èƒ½è·Ÿè¸ª

# === 2GB VPSé€‚é…æ€§ ===
âœ… å†…å­˜åˆ†é…: ç²¾ç¡®åˆ°MBçº§åˆ«è§„åˆ’
âœ… å¹¶å‘æ§åˆ¶: æœ€å¤§5ä¸ªå¹¶å‘è¯·æ±‚
âœ… ç¼“å­˜å‘½ä¸­: 60%+å‘½ä¸­ç‡ç›®æ ‡
âœ… è‡ªåŠ¨æ¸…ç†: 30ç§’é˜…åå³ç„š
âœ… å®¹é”™æ¢å¤: Dockeré‡å¯ç­–ç•¥
```

### é¡¹ç›®ä»·å€¼ï¼ˆå‡çº§ç‰ˆï¼‰

- **ğŸ¯ æŠ€æœ¯å±•ç¤º**: å±•ç¤ºç”Ÿäº§çº§ AI ç³»ç»Ÿæ¶æ„èƒ½åŠ›
- **âš¡ ç”¨æˆ·ä½“éªŒ**: æµå¼å“åº” + æ™ºèƒ½æ¨è + 3 è½®æ·±åº¦å¯¹è¯
- **ğŸš€ å­¦ä¹ ä»·å€¼**: æŒæ¡ OpenAI å®˜æ–¹æœ€ä½³å®è·µ + æ··åˆæ£€ç´¢æŠ€æœ¯
- **ğŸ’° æˆæœ¬æ•ˆç›Š**: æœˆåº¦è¿è¥æˆæœ¬ < $0.28ï¼Œæ¯æŸ¥è¯¢ < $0.001
- **ğŸ”’ å®‰å…¨å¯é **: ä¸¥æ ¼é™æµ + è¾“å…¥éªŒè¯ + é˜…åå³ç„šéšç§ä¿æŠ¤
- **ğŸ“Š å¯è§‚æµ‹æ€§**: å®Œæ•´ç›‘æ§ + ç¼“å­˜ç»Ÿè®¡ + æ€§èƒ½è¿½è¸ª
- **ğŸ—ï¸ å¯æ‰©å±•æ€§**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒåŠŸèƒ½æ¸è¿›å¼æ‰©å±•

### ç«äº‰ä¼˜åŠ¿

1. **OpenAI å®˜æ–¹æ ‡å‡†**: å®Œå…¨åŸºäºå®˜æ–¹æ–‡æ¡£å®ç°ï¼ŒæŠ€æœ¯æƒå¨æ€§
2. **ç¡¬ä»¶çº¦æŸé€‚é…**: ä¸“ä¸º 2GB VPS ä¼˜åŒ–ï¼Œèµ„æºåˆ©ç”¨ç‡æé«˜
3. **æ··åˆæ£€ç´¢åˆ›æ–°**: BM25+Vector åŒå¼•æ“ï¼Œæ£€ç´¢ç²¾åº¦ä¸šç•Œé¢†å…ˆ
4. **æˆæœ¬æ§åˆ¶ç²¾å‡†**: æ™ºèƒ½ç¼“å­˜+é™æµï¼Œè¿è¥æˆæœ¬å¯æ§å¯é¢„æµ‹
5. **é˜…åå³ç„šç‰¹è‰²**: éšç§å‹å¥½çš„å¯¹è¯è®¾è®¡ï¼Œç”¨æˆ·ä½“éªŒç‹¬ç‰¹

è¿™æ˜¯ä¸€ä¸ª**æŠ€æœ¯å…¨é¢ã€æˆæœ¬å¯æ§ã€ç”¨æˆ·å‹å¥½**çš„ç”Ÿäº§çº§ RAG èŠå¤©æœºå™¨äººæ¶æ„ã€‚
